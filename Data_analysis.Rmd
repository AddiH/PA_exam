---
title: "Data_analyse"
author: "Astrid Elmann"
date: "1/20/2022"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
pacman::p_load(tidyverse, readbulk, wesanderson, lmerTest, rstatix, ggpubr, plyr, dplyr)
```

```{r, echo=FALSE}
#+++++++++++++++++++++++++
# Function to calculate the mean and the standard deviation
  # for each group
#+++++++++++++++++++++++++
# data : a data frame
# varname : the name of a column containing the variable
  #to be summariezed
# groupnames : vector of column names to be used as
  # grouping variables
data_summary <- function(data, varname, groupnames){
  require(plyr)
  summary_func <- function(x, col){
    c(mean = mean(x[[col]], na.rm=TRUE),
      sd = sd(x[[col]], na.rm=TRUE))
  }
  data_sum<-ddply(data, groupnames, .fun=summary_func,
                  varname)
  data_sum <- rename(data_sum, c("mean" = varname))
 return(data_sum)
}
```

```{r, echo=FALSE}
# load data
with_error_neutral <- read_csv("clean_data.csv",
              col_names = TRUE,
              col_types = cols(
                timestamp = col_character() , 
                experiment_ID = col_factor(), 
                subject = col_factor(),
                r_len = col_number(),
                handedness = col_factor(),
                vision = col_factor(),
                colorvision = col_factor(),
                age = col_integer(),
                gender = col_factor(),
                nationality = col_factor(),
                r_type = col_factor(),
                IOS = col_factor(), 
                key_press = col_factor(),
                rt = col_number(),
                trial_stim = col_factor(),
                congruency = col_factor(),
                accuracy = col_factor()
              ))
```

```{r, include=FALSE}

glimpse(with_error_neutral)
```

```{r, include=FALSE}
# Make IOS ordered variable (noone chose 2 or 7)
with_error_neutral$IOS <- ordered(with_error_neutral$IOS, levels = c(1, 3, 4, 5, 6))

#with_error_neutral$IOS <- ordered(with_error_neutral$IOS, levels = c(1, 2, 3, 4, 5, 6, 7))

class(with_error_neutral$IOS)
levels(with_error_neutral$IOS)
```


```{r, echo=FALSE}
# remove error and neutral trials
d <- with_error_neutral %>% 
  filter(accuracy == "correct") %>% 
  filter(congruency != "neutral")
```

```{r}
glimpse(d)
```


# Se her!!
> Vi laver en standard joint simon task. To personer sidder ved en computer, én skal reagere når den cirkel der dukker op på skærmen er blå, den anden når cirklen er gul. Ved incongruent trials har cirklen været i modsatte side af, hvor forsøgspersonen der skal trykke sidder (Personen sidder f.eks. til højre, og cirklen dukker op i venstre side af skærmen). Congruent trials har cirklen og personen i samme side.
Vi har desuden samlet data om hvor tætte forsøgsdeltagerne føler de er på hinanden. 1 værende ikke særlig tætte, og 7 værende meget tætte.
I alt er der 2 independdent variabler:

 congruency = cirklens position, factor
 
 IOS = Personernes "tæthed", ordinal factor
 
> og én dependent:

  rt = reaktionstid i sekunder, kontunierlig skala
 


```{r, echo=FALSE}
ggplot(d,(aes( x = IOS, y = rt, fill = congruency))) + 
  geom_boxplot(position=position_dodge(.75)) +
  scale_fill_manual(values = pal) 
```

# -
# Testing hypothesis
# -
# Assumptions for anova

## Outliers

```{r}
d %>% 
  select(congruency, rt) %>% 
  group_by(congruency) %>%
  identify_outliers(rt)
```

> 5 outliers are found, but as they are not extreme (and we also already removed +-3SD) we will leave them in

## Normality
```{r}
# Build the linear model
model  <- lm(rt ~ IOS * congruency, d)
# Create a QQ plot of residuals
ggqqplot(residuals(model))
```

> Looks okay.

```{r}
# Compute Shapiro-Wilk test of normality
shapiro_test(residuals(model))
```

> P-value of 0.0003 means that it is statisticly different than a normal distribution :(

### Transform data and try again
```{r}
d$log_rt <- log(d$rt)
# Build the linear model
model  <- lm(log_rt ~ IOS * congruency, d)
# Create a QQ plot of residuals
ggqqplot(residuals(model))
# Compute Shapiro-Wilk test of normality
shapiro_test(residuals(model))
```

> Allright! we'll take the log tranformation

### Normality within groups
```{r}
ggqqplot(d, "log_rt", facet.by = "congruency")
d %>%
  group_by(congruency) %>%
  shapiro_test(log_rt)
```

```{r}
ggqqplot(d, "log_rt", facet.by = "IOS")
d %>%
  group_by(IOS) %>%
  shapiro_test(log_rt)
```


> Not everything is normal here, so we might as well go with an anova for non-parametric data. IDK what kind of test to do for two independent non-parametric data tho, so lets proceed as if the data was all normal.


## Homogneity of variance assumption
```{r}
plot(model, 1)
```

> Idk how to read this, lets look at the numbers


```{r}
d %>% levene_test(log_rt ~ IOS * congruency)
```

> it's baaaaad oh noes. We do not meet the homogneity of variance assumption

# Standard anova
```{r}
# Normal anova
stand_aov <- aov(log_rt ~ IOS * congruency, d)
summary(stand_aov)
```

>This one points to significant differences, but we don't meet the assumptions, so we can't use this. (let pretend we can tho, do the turkey test and see where we see a difference)

```{r}
turkey_test <- TukeyHSD(stand_aov)
turkey_test
```

```{r}
plot(turkey_test)
```



# Alternatives to normal anova 
```{r}
# Kruskal-Wallis anova (for non-normal data) - does not work, can only take one independent variable :'(

# kruskal.test(log_rt ~ IOS * congruency, d)
```
> what dooo??? (Should we just do the normal anova? Cause that is what we would do if we had enough data probably. It is hella sus that it is significant, it doesnt look like that when looking at the barplots with errorbars)

# -
# Post hoc
# -
# Do we even have a simon effect?

## Assumptions
### Outlier assumption
```{r}
d %>% 
  select(congruency, log_rt) %>% 
  group_by(congruency) %>%
  identify_outliers(log_rt)
```

> No outliers

### Normality assumption
```{r}
# Build the linear model
model  <- lm(log_rt ~ congruency, d)
# Create a QQ plot of residuals
ggqqplot(residuals(model))
# Compute Shapiro-Wilk test of normality
shapiro_test(residuals(model))
```

> Data is closer to (but isn't) normal with log transformation, so lets use the Kruskal-Wallis anova. (that also doesn't have the assumption of homogneity of variance)


### Anova
```{r}
#Kruskal-Wallis anova, that is okay with non-normal data:
kruskal.test(rt ~ congruency, d)
```
> yay! at least we see the simon effect.
WAIT THIS SHOULD JUST BE A TETESTSTETTS

# Difference in relatioinships type
```{r, echo=FALSE}
ggplot(d,(aes( x = r_type, y = rt, fill = congruency))) + 
  geom_boxplot(position=position_dodge(.75)) +
  scale_fill_manual(values = pal) 
```


## Assumptions
### Outlier assumption

```{r}
d %>% 
  select(r_type, rt) %>% 
  group_by(r_type) %>%
  identify_outliers(rt)
```

### Normality

```{r}
# Build the linear model
model  <- lm(rt ~ r_type * congruency, d)
# Create a QQ plot of residuals
ggqqplot(residuals(model))
```

```{r}
# Compute Shapiro-Wilk test of normality
shapiro_test(residuals(model))
```

### Homogneity of variance assumption

```{r}
plot(model, 1)
```

```{r}
d %>% levene_test(log_rt ~ r_type * congruency)
```

```{r}
# Normal anova
stand_aov <- aov(log_rt ~ r_type * congruency, d)
summary(stand_aov)
```

```{r}
turkey_test <- TukeyHSD(stand_aov)
turkey_test
```





# Demographics
```{r}
agedf <- d %>% 
  group_by(subject) %>% 
  summarise(mean = mean(age))

mean(d$age)

agev <- c(80, 81, 20, 22, 27, 31, 23, 21, 24, 25, 58, 56)

mean(agev)
sd(agev)

unique(d$IOS)
```


```{r}
d$IOS <- as.numeric(d$IOS)
class(d$IOS)

d_group <- d %>% 
    group_by(IOS, congruency) 

d_group %>% 
  dplyr::summarize(rt_mean = mean(rt))
```


```{r}
mtcars %>%
  group_by(cyl) %>%
  dplyr::summarise(mean = mean(disp), n = n())
```



